{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:19:25.204653800Z",
     "start_time": "2023-05-17T05:19:21.382454600Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:19:25.216770300Z",
     "start_time": "2023-05-17T05:19:25.206657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir = \"Alzheimer_s Dataset\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:19:25.230421600Z",
     "start_time": "2023-05-17T05:19:25.217766500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n"
     ]
    }
   ],
   "source": [
    "data_dir += \"/train\"\n",
    "print(os.listdir(data_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:19:25.372424600Z",
     "start_time": "2023-05-17T05:19:25.227418200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV bindings requires \"numpy\" package.\n",
      "Install it via command:\n",
      "    pip install numpy\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core.multiarray'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m temp_dir \u001B[38;5;241m=\u001B[39m data_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/MildDemented\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(temp_dir):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cv2\\__init__.py:12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmultiarray\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOpenCV bindings requires \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m package.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'numpy.core.multiarray'"
     ]
    }
   ],
   "source": [
    "temp_dir = data_dir + \"/MildDemented\"\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for img in os.listdir(temp_dir):\n",
    "    img_array = cv2.imread(os.path.join(temp_dir, img))\n",
    "    #print(img_array)\n",
    "    plt.imshow(img_array)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.345424700Z"
    }
   },
   "outputs": [],
   "source": [
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.346428700Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.347428800Z"
    }
   },
   "outputs": [],
   "source": [
    "# to augment images and create more samples\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(brightness_range=[0.8, 1.2], zoom_range=[0.99, 1.01], horizontal_flip=True, fill_mode=\"constant\", data_format=\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.351426Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_gen = image_generator.flow_from_directory(directory=data_dir, target_size=(176, 176), batch_size=6500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.354431200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.357426400Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.360425Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_images(generator,y_pred=None):\n",
    "    #Input: An image generator, predicted labels (optional)\n",
    "    #Output: Displays a grid of 9 random images with lables\n",
    "        \n",
    "    #get image lables\n",
    "    labels =dict(zip([0,1,2,3], classes))\n",
    "    \n",
    "    #get a batch of images\n",
    "    x,y = generator.next()\n",
    "    \n",
    "    #display a grid of 9 images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    if y_pred is None:\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            idx = randint(0, 5121) #because there are a total of 5121 images\n",
    "            plt.imshow(x[idx].astype('uint8')) #This is done because of the reason that if the color intensity is a float,\n",
    "            #then matplotlib expects it to range from 0 to 1. If an int, then it expects 0 to 255.\n",
    "            #So we can either force all the numbers to int or scale them all by 1/255 or use the .astype function to cast the object onto our specified dtype.\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Class: {}\".format(labels[np.argmax(y[idx])]))\n",
    "                                                     \n",
    "    else:\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(x[i].astype('uint8'))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Actual: {} \\nPredicted: {}\".format(labels[np.argmax(y[i])],labels[y_pred[i]]))\n",
    "    \n",
    "# Display Train Images\n",
    "show_images(train_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.364426Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = train_data_gen.next()\n",
    "print(train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T05:19:25.368426200Z"
    }
   },
   "outputs": [],
   "source": [
    "for alzheimers_class in classes:\n",
    "    container = []\n",
    "    temp_dir = data_dir + \"/\" + alzheimers_class\n",
    "    for file in os.listdir(temp_dir):\n",
    "        container.append(file)\n",
    "    print(alzheimers_class, \": \", len(container))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of data samples for 'ModerateDemented' is considerably low, compared to the other classes. A clear contrast between the sizes of the training classes shows how imbalanced our classes are. So we'll need to perform over-sampling of the data using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "\n",
    "sm = imblearn.over_sampling.SMOTE(random_state=42)\n",
    "train_data, train_labels = sm.fit_resample(train_data.reshape(-1, 176 * 176 * 3), train_labels)\n",
    "train_data = train_data.reshape(-1, 176, 176, 3)\n",
    "print(train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to use multiple types of blocks, multiple times. So I'm just going to define functions for each type of blocks and call them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(filters, act='relu'):    \n",
    "    block = tf.keras.Sequential()\n",
    "    block.add(tf.keras.layers.Conv2D(filters, 3, activation=act, padding='same'))\n",
    "    block.add(tf.keras.layers.Conv2D(filters, 3, activation=act, padding='same'))\n",
    "    block.add(tf.keras.layers.BatchNormalization())\n",
    "    block.add(tf.keras.layers.MaxPool2D())\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_block(units, dropout_rate, act='relu'):    \n",
    "    block = tf.keras.Sequential()\n",
    "    block.add(tf.keras.layers.Dense(units, activation=act))\n",
    "    block.add(tf.keras.layers.BatchNormalization())\n",
    "    block.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(act='relu'):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(*[176, 176], 3)),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation=act, padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation=act, padding='same'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        conv_block(32),\n",
    "        conv_block(64),\n",
    "        conv_block(128),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        conv_block(256),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        norm_block(512, 0.7),\n",
    "        norm_block(128, 0.5),\n",
    "        norm_block(64, 0.3),\n",
    "        tf.keras.layers.Dense(4, activation='softmax') #Output Layer       \n",
    "    ], name = \"cnn_model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = construct_model()\n",
    "\n",
    "METRICS = [tf.keras.metrics.CategoricalAccuracy(name='acc'),\n",
    "           tf.keras.metrics.AUC(name='auc')]\n",
    "    \n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.CategoricalCrossentropy(),\n",
    "              metrics=METRICS)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_labels, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['auc', 'loss', 'acc']):\n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir1 = data_dir[:19]+'/test'\n",
    "data_dir1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen = image_generator.flow_from_directory(directory=data_dir1, target_size=(176, 176), batch_size=6500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = test_data_gen.next()\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(test_data, test_labels)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundoff(arr):\n",
    "    \"\"\"To round off according to the argmax of each predicted label array. \"\"\"\n",
    "    arr[np.argwhere(arr != arr.max())] = 0\n",
    "    arr[np.argwhere(arr == arr.max())] = 1\n",
    "    return arr\n",
    "\n",
    "for labels in predicted_test_labels:\n",
    "    labels = roundoff(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.metrics.classification_report(test_labels, predicted_test_labels, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls = np.argmax(predicted_test_labels, axis=1)\n",
    "test_ls = np.argmax(test_labels, axis=1)\n",
    "\n",
    "conf_arr = sklearn.metrics.confusion_matrix(test_ls, pred_ls)\n",
    "plt.figure(figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "import seaborn as sns\n",
    "ax = sns.heatmap(conf_arr, cmap='Reds', annot=True, fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.title('Alzheimer\\'s Disease Diagnosis')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from flask import Flask, flash, request, redirect, url_for, render_template\n",
    "from werkzeug.utils import secure_filename\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "ASSET_FOLDER = 'static/assets/'\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['ASSET_FOLDER'] = ASSET_FOLDER\n",
    "\n",
    "github_icon = os.path.join(app.config['ASSET_FOLDER'], 'github.png')\n",
    "linkedin_icon = os.path.join(app.config['ASSET_FOLDER'], 'linkedin.png')\n",
    "instagram_icon = os.path.join(app.config['ASSET_FOLDER'], 'instagram.png')\n",
    "twitter_icon = os.path.join(app.config['ASSET_FOLDER'], 'twitter.png')\n",
    "bg_video = os.path.join(app.config['ASSET_FOLDER'], 'pexels-tima-miroshnichenko-6010766.mp4')\n",
    "    \n",
    "@app.route('/')\n",
    "def upload_form():\n",
    "    return render_template('index.html', type=\"\", githubicon=github_icon, linkedinicon=linkedin_icon, instagramicon=instagram_icon, twittericon=twitter_icon, bgvideo=bg_video)\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def upload_image():\n",
    "    file = request.files['file']\n",
    "    print(file.filename)\n",
    "    file.save(os.path.join(\"static/uploads\", secure_filename(file.filename)))\n",
    "\n",
    "    imvar = tf.keras.preprocessing.image.load_img(os.path.join(\"static/uploads\", secure_filename(file.filename))).resize((176, 176))\n",
    "    imarr = tf.keras.preprocessing.image.img_to_array(imvar)\n",
    "    imarr = np.array([imarr])\n",
    "    model2 = tf.keras.models.load_model(\"model\")\n",
    "    impred = model2.predict(imarr)\n",
    "\n",
    "    def roundoff(arr):\n",
    "        \"\"\"To round off according to the argmax of each predicted label array. \"\"\"\n",
    "        arr[np.argwhere(arr != arr.max())] = 0\n",
    "        arr[np.argwhere(arr == arr.max())] = 1\n",
    "        return arr\n",
    "\n",
    "    for classpreds in impred:\n",
    "        impred = roundoff(classpreds)\n",
    "    \n",
    "    classcount = 1\n",
    "    for count in range(4):\n",
    "        if impred[count] == 1.0:\n",
    "            break\n",
    "        else:\n",
    "            classcount+=1\n",
    "    \n",
    "    classdict = {1: \"Mild Dementia\", 2: \"Moderate Dementia\", 3: \"No Dementia, Patient is Safe\", 4: \"Very Mild Dementia\"}\n",
    "    print(classdict[classcount])\n",
    "\n",
    "    c = 'xyz'\n",
    "    return render_template('index.html', type=\"Patient is suffering from \"+str(classdict[classcount]), githubicon=github_icon, linkedinicon=linkedin_icon, instagramicon=instagram_icon, twittericon=twitter_icon, bgvideo=bg_video)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42d631c97d3a7942c4c370955af34a4195632f6be3582dbc391d521a11ed78e3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
